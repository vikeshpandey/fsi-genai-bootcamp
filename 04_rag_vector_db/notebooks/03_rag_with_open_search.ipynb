{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question & Answering with Amazon Bedrock using LangChain & Vector Search\n",
    "> *This notebook should work well with the **`conda_python3`** kernel in SageMaker Studio on ml.t3.medium instance*\n",
    "\n",
    "---\n",
    "\n",
    "Previously, we used the Anthropic Claude model in Amazon Bedrock to demonstrate a basic Question Answering (QA) system, and learned the value of grounding a model with additional context before generating a response. In the previous notebook, we had to manually provide the model with relevant data and context ourselves. However, this approach is not fit for enterprise-level QA systems where there could be hundreds of thousands of large documents.\n",
    "\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "We can improve upon this process by implementing an architecture called retrieval augmented generation (RAG). RAG retrieves data from outside the LLM's training data sources and augments the prompts by adding the relevant retrieved data as context. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, without needing to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
    "\n",
    "## Solution\n",
    "\n",
    "In this notebook, we augment LLM responses to user queries by implementing RAG using context from external documents. First, we process documents and store these into a vector store. Next, we search the vector store using the user's question, and return relevant data as external context to the LLM. Finally, the LLM generates an answer to the user's question based on the new context provided.\n",
    "\n",
    "We will walk through implementing the following two patterns: Question Answering (QA) and Conversational AI with conversation memory. \n",
    "\n",
    "Let’s break down the solution a little further. \n",
    "\n",
    "### Prepare documents for search\n",
    "![Documents](./images/embeddings_lang.png)\n",
    "\n",
    "First, the documents must be processed and then indexed in a vector store.\n",
    "- Load the documents from our directory\n",
    "- Process the documents by splitting them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using an embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "\n",
    "### Respond to the user’s question\n",
    "![Question](./images/chatbot_lang.png)\n",
    "\n",
    "Once the vector store is indexed with documents and embeddings, we can search for text relevant to the question being asked. The relevant chunks are sent to the model as additional context, where the model will then generate the answer.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v1\",\n",
    "    \"amazon.titan-embed-text-v2:0\",\n",
    "    \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    \"amazon.rerank-v1:0\"\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from rich import print as rprint\n",
    "from rich.markdown import Markdown\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LangChain\n",
    "\n",
    "LangChain provides convenient integrations with Amazon Bedrock and other services like vector stores and retrievers. We begin with instantiating the large language model (LLM) and the embeddings model. We are using Anthropic Claude models for text generation and Amazon Titan Embeddings G1 - Text for text embedding.\n",
    "\n",
    "Note: Amazon Bedrock offers a choice of high-performing foundation models (FMs). You can replace the value for `model_id` with one of the available [model IDs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) as follows. Some models have different requirements for inputs such as prompt format. As of this writing, all models are supported in the US West (Oregon, us-west-2) Region. If you are using another AWS Region, check the latest [model support by AWS Region](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html).\n",
    "\n",
    "```python\n",
    "llm = ChatBedrock(model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\", ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_aws.chat_models import ChatBedrock\n",
    "from langchain.load.dump import dumps\n",
    "\n",
    "# Instantiate the LLM\n",
    "\n",
    "model_id = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    model_kwargs={\"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "# Instantiate the Amazon Titan Embeddings G1 - Text embeddings model\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\" # change this model ID to use another embeddings model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase Introduction - Model Risk and Model Governance Assistant\n",
    "In this notebook we will learn the application of RAG through a practical example. The use case we will be working on is a Model Risk and Model Governance Assistant. This assistant will help users understand the risks associated with deploying machine learning models in production. The assistant will provide information on the following topics:\n",
    "- Model Risk Management\n",
    "- Model Governance\n",
    "- Regulatory Compliance\n",
    "- Model Monitoring\n",
    "- Model Validation\n",
    "- And more\n",
    "\n",
    "We will use some publicly available regulatory guideline documents to serve as the source for our RAG solution. You can vew the documents in the `../data/model_risk` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We will load the documents with the help of [PyPDF in LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf).\n",
    "\n",
    "We will utilize a few different techniques when loading the documents that will help improve the retrieval quality.\n",
    "\n",
    "#### Outline based splitting\n",
    "By default LangChain's `PyPDFLoader` will break each document up into pages. We could then potentially use a chunking strategy such as `RecursiveCharacterTextSplitter` to further break down the pages into smaller chunks. \n",
    "However, this could lead to suboptimal results if the most relevant information we are looking for is split across multiple pages. Instead, we will split the documents into sections based on the documents own table of contents. The implementation for this approach is provided in the rag_utils.outline_parser module [(source)](./rag_utils/outline_parser.py).\n",
    "Note that this approach only works on PDFs that contain a table of contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from rag_utils.outline_parser import PyPDFOutlineParser\n",
    "\n",
    "docs_path = Path(\"../data/model_risk\")\n",
    "doc_files = list(docs_path.glob(\"*.pdf\"))\n",
    "\n",
    "section_chunks = []\n",
    "\n",
    "for doc_path in doc_files:\n",
    "    loader = PyPDFLoader(file_path=doc_path.as_posix())\n",
    "    loader.parser = PyPDFOutlineParser()\n",
    "    sections = loader.load()\n",
    "    for sec in sections:\n",
    "        sec.metadata.update({\"file\": doc_path.name})\n",
    "    \n",
    "    section_chunks += sections\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each section chunk now contains a the contents and metadata associated with that section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test out our embedding model on a single section to see what an embedding looks like below. These embeddings could be generated for the entire corpus of documents and stored in a vector store for easy retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_embedding = bedrock_embeddings.embed_query(section_chunks[0].page_content)\n",
    "modelId = bedrock_embeddings.model_id\n",
    "rprint(\"Embedding model Id :\", modelId)\n",
    "rprint(\"Sample embedding of a document chunk: \", sample_embedding[:10])\n",
    "rprint(\"Size of the embedding: \", len(sample_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vector store\n",
    "In this workshop we will use the ***vector engine for Amazon OpenSearch Serverless.***\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application — without impacting data ingestion.\n",
    "\n",
    "The code block below will provision an OpenSearch Serverless collection and create the vector store that we will use to store the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from rag_utils.oss_utils import get_aws_auth, get_host\n",
    "\n",
    "auth = get_aws_auth()\n",
    "host = get_host(\"bedrock-workshop-rag\")\n",
    "host = f\"{host}:443\"\n",
    "\n",
    "index_name = \"model-risk-index\"\n",
    "\n",
    "vector_db = OpenSearchVectorSearch(\n",
    "    index_name=index_name,\n",
    "    embedding_function=bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 7200,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    engine=\"faiss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\", \"\\n\\n\"], chunk_size=2000, chunk_overlap=250\n",
    ")\n",
    "\n",
    "split_docs = child_splitter.split_documents(section_chunks)\n",
    "\n",
    "if vector_db.client.indices.exists(index=index_name):\n",
    "    rprint(\"Index already exists. Checking if documents are already added.\")\n",
    "    doc_count = vector_db.client.count(index=index_name)[\"count\"]\n",
    "    if doc_count != len(split_docs):\n",
    "        rprint(f\"Index contains less than {len(split_docs)} documents. Adding documents.\")\n",
    "        vector_db.delete_index()\n",
    "        vector_db.add_documents(split_docs)\n",
    "    else:\n",
    "        rprint(f\"Index contains {doc_count} documents. No need to add documents.\")\n",
    "\n",
    "else:\n",
    "    rprint(\"Creating index and adding documents.\")\n",
    "    vector_db.add_documents(split_docs)\n",
    "    rprint(\"Documents added to the index. Waiting for verification\")\n",
    "    doc_count = vector_db.client.count(index=index_name)[\"count\"]\n",
    "    max_wait = 120\n",
    "    while doc_count != len(split_docs):\n",
    "        rprint(\"Documents not yet reflected in index. Waiting for 20 seconds.\")\n",
    "        time.sleep(20)\n",
    "        doc_count = vector_db.client.count(index=index_name)[\"count\"]\n",
    "        max_wait -= 20\n",
    "        if max_wait <= 0:\n",
    "            rprint(\"Documents not reflected in index in 120 seconds. Exiting.\")\n",
    "            break\n",
    "    rprint(f\"Documents added to the index. Index contains {doc_count} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Searching the vector store\n",
    "Let's explore the various ways that we can query the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search methods\n",
    "[Semantic search](https://opensearch.org/docs/latest/search-plugins/semantic-search/) considers the context and intent of a query. In OpenSearch, semantic search is facilitated by neural search with text embedding models. Semantic search creates a dense vector (a list of floats) and ingests data into a k-NN index.\n",
    "\n",
    "Short for k-nearest neighbors, the k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors.  To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points. We will explore some of the distance functions later in this lab.\n",
    "\n",
    "The k-NN plugin supports three different methods for obtaining the k-nearest neighbors from an index of vectors:\n",
    "- Approximate k-NN\n",
    "- Script Score k-NN\n",
    "- Painless extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate k-NN search\n",
    "Standard k-NN search methods compute similarity using a brute-force approach that measures the nearest distance between a query and a number of points, which produces exact results. This works well in many applications. However, in the case of extremely large datasets with high dimensionality, this creates a scaling problem that reduces the efficiency of the search. [Approximate k-NN search](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/) methods can overcome this by employing tools that restructure indexes more efficiently and reduce the dimensionality of searchable vectors. Using this approach requires a sacrifice in accuracy but increases search processing speeds appreciably.\n",
    "\n",
    "Of the three search methods the k-NN plugin provides, this method offers the best search scalability for large datasets. This approach is the preferred method when a dataset reaches hundreds of thousands of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search query\n",
    "query = \"What can be considered a model?\"\n",
    "\n",
    "# Search for the 3 most relevant documents\n",
    "results = vector_db.similarity_search(query, k=3)\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact k-NN with scoring script\n",
    "The k-NN plugin implements the OpenSearch [score script](https://opensearch.org/docs/latest/search-plugins/knn/knn-score-script/) plugin that you can use to find the exact k-nearest neighbors to a given query point.\n",
    "\n",
    "Because the score script approach executes a brute force search, it doesn’t scale as well as the approximate approach. This approach is preferred for searches over smaller bodies of documents or when a pre-filter is needed. Using this approach on large indexes may lead to high latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = vector_db.similarity_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    is_appx_search=False,\n",
    "    search_type=\"script_scoring\"\n",
    ")\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Painless Scripting extensions (exact)\n",
    "Currently, the vector engine for Amazon OpenSearch serverless supports the approximate k-NN search and scoring script search methods. Below is an example of the [painless scripting](https://opensearch.org/docs/latest/search-plugins/knn/painless-functions/) search method on an Amazon OpenSearch service for your reference.\n",
    "\n",
    "Similar to the k-NN Script Score, you can use this method to perform a brute force, exact k-NN search across an index. This approach has slightly slower query performance compared to the k-NN scoring script. If your use case requires more customization over the final score, you should use this approach over k-NN scoring script.\n",
    "\n",
    "```python\n",
    "results = docsearch.similarity_search(\n",
    "    query, \n",
    "    is_appx_search=False,\n",
    "    search_type=\"painless_scripting\"\n",
    ")\n",
    "\n",
    "print(dumps(results, pretty=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact k-NN search with filters\n",
    "You can apply [k-NN search with filters](https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/) with either the  scoring script or painless extension search methods. Filters can greatly reduce the number of vectors to be searched. Using the k-NN score script, you can apply a filter on an index before executing the nearest neighbor search (sometimes referred to as a pre-filter search). This is useful for dynamic search cases where the index body may vary based on other conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the acceptable model evaluation techniques?\"\n",
    "\n",
    "# filter on a specific document\n",
    "pre_filter = {\"bool\": {\"filter\": {\"match\": {\"metadata.file\": \"sr1107a1.pdf\"}}}}\n",
    "\n",
    "# Pre-filter results\n",
    "results = vector_db.similarity_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    is_appx_search=False,\n",
    "    search_type=\"script_scoring\",\n",
    "    pre_filter=pre_filter   \n",
    ")\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaces - similarity or distance measures\n",
    "\n",
    "Approximate Search through OpenSearch supports the following similarity or distance measures:\n",
    "\n",
    "**Cosine similarity** – The cosine of the angle between two vectors in a vector space.\n",
    "\n",
    "**Euclidean distance** – The straight-line distance between points.\n",
    "\n",
    "**L1 (Manhattan) distance** – The sum of the differences of all of the vector components. L1 distance measures how many orthogonal city blocks you need to traverse from point A to point B.\n",
    "\n",
    "**L-infinity (chessboard) distance** – The number of moves a King would make on an n-dimensional chessboard. It’s different than Euclidean distance on the diagonals—a diagonal step on a 2-dimensional chessboard is 1.41 Euclidean units away, but 2 L-infinity units away.\n",
    "\n",
    "**Inner product** – The product of the magnitudes of two vectors and the cosine of the angle between them. Usually used for natural language processing (NLP) vector similarity.\n",
    "\n",
    "We can specify the distance measure in the `space_type` parameter when we load our documents as seen below.\n",
    "\n",
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"l2\", # Options are: “l2”, “l1”, “cosinesimil”, “linf”, “innerproduct”; default: “l2”\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Engines and algorithms\n",
    "The Approximate k-NN search methods leveraged by OpenSearch use approximate nearest neighbor (ANN) algorithms from the [NMSLIB](https://github.com/nmslib/nmslib), [FAISS](https://github.com/facebookresearch/faiss), and [Lucene](https://lucene.apache.org/) libraries to power k-NN search.\n",
    "\n",
    "The engine details are as follows:\n",
    "\n",
    "- Non-Metric Space Library (NMSLIB) – NMSLIB implements the HNSW ANN algorithm\n",
    "- Facebook AI Similarity Search (FAISS) – FAISS implements both HNSW and IVF ANN algorithms\n",
    "- Lucene – Lucene implements the HNSW algorithm\n",
    "\n",
    "Each of the three engines used for approximate k-NN search has its own attributes that make one more sensible to use than the others in a given situation. In general, NMSLIB and FAISS should be selected for large-scale use cases. Lucene is a good option for smaller deployments, but offers benefits like smart filtering where the optimal filtering strategy—pre-filtering, post-filtering, or exact k-NN—is automatically applied depending on the situation.\n",
    "\n",
    "We can specify the engine as shown below.\n",
    "\n",
    "Note: As of this writing, Amazon OpenSearch Serverless vector search collections don't support the Apache Lucene ANN engine. Vector search collections only support the HNSW algorithm with FAISS and do not support IVF and IVFQ. Please check the updated [limitations](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html#serverless-vector-limitations).\n",
    "\n",
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\", # Options are: “nmslib”, “faiss”, “lucene”; default: “nmslib”\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For HNSW, we can tune the m, ef_construction, and ef_search parameters to achieve our desired trade-off:**\n",
    "\n",
    "**m** – Controls the maximum number of edges a node in a graph can have. Because each node has to store all of its edges, increasing this value will increase the memory footprint, but also increase the connectivity of the graph, which will improve recall.\n",
    "\n",
    "**ef_construction** – Controls the size of the candidate queue for edges when adding a node to the graph. Increasing this value will increase the number of candidates to consider, which will increase the index latency. However, because more candidates will be considered, the quality of the graph will be better, leading to better recall during search.\n",
    "\n",
    "**ef_search** – Similar to ef_construction, it controls the size of the candidate queue for graph traversal during search. Increasing this value will increase the search latency, but will also improve the recall.\n",
    "\n",
    "In general, we chose configurations that gradually increase the parameters, as detailed in the following table.\n",
    "\n",
    "![](./images/hnsw_parameters.png)\n",
    "\n",
    "Below is an example using the **FAISS** engine and providing a parameter configuration that provides a balance between latency, memory, and recall (see table above).\n",
    "\n",
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"l2\",\n",
    "    m=16, # maximum number of edges\n",
    "    ef_construction=128, # size of the candidate queue for edges\n",
    "    ef_search=128 # size of the candidate queue for graph traversal\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum marginal relevance search (MMR)\n",
    "If you’d like to look up for some similar documents, but you’d also like to receive diverse results, MMR is a method you should consider. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we fetch 10 results but then return the top 3 most diverse\n",
    "results = vector_db.max_marginal_relevance_search(query, k=3, fetch_k=10)\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrating RAG using LangChain\n",
    "Now that we can query our vector database for documents, we can retrieve data from outside of a large language model's training data sources and augment our prompts by adding the relevant retrieved data in context.\n",
    "\n",
    "We can use LangChain to build applications that read data from stored internal documents and summarize them into conversational responses. We can create a Retrieval Augmented Generation (RAG) workflow that introduces new information to the language model during prompting. Implementing context-aware workflows like RAG reduces model hallucination and improves response accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single turn generative question answering\n",
    "\n",
    "Let's start with a simple example where given a user query we retrieve relevant documents from the vector store and use the retrieved documents as context to generate a response.\n",
    "\n",
    "We'll construct a prompt template that will take the user's question and the retrieved documents as context and generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. \n",
    "If the context does not provide sufficient information to answer the question, politely indicate that you are unable to assist. \n",
    "Only answer questions related to model risk and model governance.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "retriever = vector_db.as_retriever(k=5)\n",
    "\n",
    "# in the first step we retrieve the context and pass through the input question\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    \n",
    ")\n",
    "\n",
    "# In the subsequent steps pass the context and question to the prompt, send the prompt to the llm and parse the output as a string\n",
    "chain = setup_and_retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try this with our earlier queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What can be considered a model?\"\n",
    "\n",
    "response = chain.invoke(query)\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are some acceptable model evaluation techniques?\"\n",
    "\n",
    "response = chain.invoke(query)\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RAG workflow\n",
    "The example provides a naive implementation of RAG. While it works for demonstration purposes, it has a number of limitations:\n",
    "- The are no guardrails in place to ensure that the model does not engage in any topics other than model risk and governance\n",
    "- The user's question may not be the most optimal search query for the vector store.\n",
    "- The workflow is stateless and not conversational. User is not able to ask follow up questions\n",
    "- Retrieval is based purely on semantic similarity. There is no filtering or ranking of the retrieved documents.\n",
    "\n",
    "We'll address these limitations by introducing a few more components to our workflow.\n",
    "- [**LangGraph**](https://www.langchain.com/langgraph): A python library for building custom LLM and Agent based workflows. LangGraph allows us to express a workflow as a finite state machine (FSM) making it possible to orchestrate LLM driven steps, hardcoded logic, and human input in the loop workflows.\n",
    "- [**BM25 Retrieval**](https://en.wikipedia.org/wiki/Okapi_BM25): A retrieval method that uses a probabilistic model to rank documents based on their relevance to a query. BM25 is a popular choice for information retrieval tasks and can be used as a complement to semantic search methods.\n",
    "- [**Hybrid Retrieval**](https://python.langchain.com/docs/how_to/ensemble_retriever/): Will combine the BM25 and FAISS retrievers to provide a more robust retrieval method.\n",
    "- [**Amazon Reran Model**](https://docs.aws.amazon.com/bedrock/latest/userguide/rerank.html): A model that can be used to re-rank the retrieved documents based on their relevance to the query. This will help us filter out any irrelevant documents and improve the quality of the context provided to the LLM.\n",
    "- [**Bedrock Session API**](https://docs.aws.amazon.com/bedrock/latest/userguide/sessions.html): A new API that allows us to maintain a session with the LLM. This will help us maintain conversation state and allow for follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "from langchain_aws.document_compressors.rerank import BedrockRerank\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from rag_utils.oss_utils import OpenSearchBM25Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the improvements to the retrieval workflow. We'll start by creating a new retriever that combines the BM25 and FAISS retrievers. We'll then use this retriever to retrieve documents based on the user's query. Finally, we'll use the Rerank model to re-rank the retrieved documents based on their relevance to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for lexical search\n",
    "bm_25_retriever = OpenSearchBM25Retriever(client=vector_db.client, index_name=index_name, k=10)\n",
    "\n",
    "# used for semantic search\n",
    "semantic_search_retriever = vector_db.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# combine results from both retrievers\n",
    "# since k was set to 10 for both retrievers, the fusion retriever will return 20 results\n",
    "fusion_retriever = EnsembleRetriever(\n",
    "    retrievers=[semantic_search_retriever, bm_25_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\")\n",
    "\n",
    "# reranker model\n",
    "reranker = BedrockRerank(\n",
    "    model_arn=\"arn:aws:bedrock:us-west-2::foundation-model/amazon.rerank-v1:0\",\n",
    "    client=bedrock_agent_runtime,\n",
    "    region_name=\"us-west-2\",\n",
    "    top_n=5, # the top 5 ranked results from the fusion retriever will be returned\n",
    ")\n",
    "\n",
    "# rerank the combined results from the fusion retriever\n",
    "rerank_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=fusion_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the improved retriever\n",
    "query = \"What are some acceptable model evaluation techniques?\"\n",
    "results = rerank_retriever.invoke(query)\n",
    "rprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can start defining our workflow using LangGraph. We'll start by defining the states and transitions for our workflow. We'll then define the steps that will be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State is often defined a TypedDict\n",
    "class RAGState(TypedDict):\n",
    "    messages: Annotated[list, add_messages] # contains the conversation history, new messages are appended to the list\n",
    "    search_query: str                       # the search query generated by the model\n",
    "    context_summary: str                    # summary of the conversation   \n",
    "    search_results: list                    # the search results from the retriever\n",
    "    input_validated: bool                   # whether the input has been validated or not\n",
    "\n",
    "\n",
    "# these data models are used to define the outputs of the different steps in the pipeline\n",
    "\n",
    "# output of the input validation step\n",
    "class InputValidator(BaseModel):\n",
    "    input_validated: bool = Field(description=\"Whether the input is validated or not.\")\n",
    "\n",
    "# output of the search query generation step\n",
    "class SearchQuery(BaseModel):\n",
    "    query: str = Field(description=\"The search query to be used for retrieval.\")\n",
    "    context_summary: str = Field(description=\"The summary of the context of what is being asked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes in the workflow can be defined as functions. Each function takes in the current state, performs some operations, and returns the updated state. Think of the state as a car being assembled in a factory. Each function is a step in the assembly line that takes the car from one state to another. The final state is the fully assembled car, ready to be driven off the lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple chain to validate if the users input is on topic\n",
    "# this can be extended to include more complex validation logic\n",
    "def validate_user_input(state: RAGState) -> Literal[\"formulate_search_query\", \"generate_answer\"]:\n",
    "    \"\"\"\n",
    "    Validate the user input to ensure it is a valid search query.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    system_prompt = \"You are the first line of defense for a chatbot that answers questions about model risk and model governance regulations. \" \\\n",
    "                    \"Your job is to validate the user's input and ensure it is relevant to the topic.\" \\\n",
    "                    \"Use the available tool to validate the input and return the result.\" \n",
    "    \n",
    "    validation_chain = llm.bind_tools(tools=[InputValidator]) | PydanticToolsParser(tools=[InputValidator])\n",
    "\n",
    "    result = validation_chain.invoke([(\"system\", system_prompt)] + state[\"messages\"])[0]\n",
    "\n",
    "    # choose the next step based on the validation result\n",
    "    if result.input_validated:\n",
    "        # if valid input, we can proceed to the next step\n",
    "        return \"formulate_search_query\"\n",
    "    else:\n",
    "        # if invalid input, we can jump to the last step where a canned response is provided\n",
    "        return \"generate_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the conversation context, this step generates to help get better search results\n",
    "def formulate_search_query(state: RAGState) -> RAGState:\n",
    "    \"\"\"Formulate the search query based on the user question.\"\"\"\n",
    "    \n",
    "    query = state[\"messages\"][-1].content\n",
    "    \n",
    "    prompt = \"You are assisting with matters related to model risk and model governance. \" \\\n",
    "            \"You have access to a comprehensive database of regulations and best practices\" \\\n",
    "            \"as they relate to model risk and model governance. \" \\\n",
    "            \"Based on your expertise, help refine the user's question into a search query that can be used to retrieve relevant information.\" \\\n",
    "            \"Given the prior interaction, please provide a search query that is relevant to the topic.\" \\\n",
    "            \"Additionally, if the user's question is a follow-up question based on information from previous interactions, please take that into account.\" \\\n",
    "            \"Please provide a brief summary of the context of what is being asked.\" \\\n",
    "            \"Use the available tool to record the search query.\" \\\n",
    "\n",
    "    query_generation_chain  = llm.bind_tools([SearchQuery]) | PydanticToolsParser(tools=[SearchQuery])\n",
    "    response = query_generation_chain.invoke([(\"system\", prompt)] + state[\"messages\"])\n",
    "    query = response[0].query\n",
    "    context_summary = response[0].context_summary\n",
    "\n",
    "    # the updated state is returned as a dictionary \n",
    "    return {\"search_query\": query, \"input_validated\": True, \"context_summary\": context_summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the search using the search query generated in the previous step\n",
    "def search(state: RAGState) -> RAGState:\n",
    "    \"\"\"Perform a search using the formulated search query.\"\"\"\n",
    "    \n",
    "    search_query = state[\"search_query\"]\n",
    "    print(\"Search query: \", search_query)\n",
    "    results = rerank_retriever.invoke(search_query)\n",
    "    return {\"search_query\": search_query, \"search_results\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: RAGState) -> RAGState:\n",
    "    \"\"\"Generate an answer based on the search results.\"\"\"\n",
    "\n",
    "    # return the canned response if the input is not valid\n",
    "    if state[\"input_validated\"] == False:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"I'm sorry but I cannot assist with that.\")],\n",
    "        }\n",
    "\n",
    "    search_results = state[\"search_results\"]\n",
    "    if not search_results:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"I'm sorry I could not find any relevant information.\")],\n",
    "        }\n",
    "\n",
    "    search_results = \"\\n\".join([doc.page_content for doc in search_results])\n",
    "    question = state[\"messages\"][-1].content\n",
    "    context_summary = state[\"context_summary\"]\n",
    "\n",
    "    prompt = f\"\"\"Answer the question based only on the following context. \n",
    "    If the context does not provide sufficient information to answer the question, politely indicate that you are unable to assist. \n",
    "    Only answer questions related to model risk and model governance.\n",
    "\n",
    "    <search_results>\n",
    "    {search_results}\n",
    "    </search_results>\n",
    "\n",
    "    The context in which the question is being asked is:\n",
    "    <context_summary>\n",
    "    {context_summary}\n",
    "    </context_summary>\n",
    "\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "\n",
    "    response_message = llm.invoke(prompt)\n",
    "    return {\n",
    "        \"messages\": [response_message],\n",
    "        \"search_query\": state[\"search_query\"],\n",
    "        \"search_results\": state[\"search_results\"],\n",
    "        \"input_validated\": False,\n",
    "        \"context_summary\": context_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of the components are in place, we can start defining our workflow. We'll start by defining the nodes in the workflow, followed by the transitions between the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(RAGState)\n",
    "\n",
    "graph_builder.add_node(\"formulate_search_query\", formulate_search_query)\n",
    "graph_builder.add_node(\"search\", search)\n",
    "graph_builder.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "graph_builder.add_conditional_edges(START, validate_user_input)\n",
    "graph_builder.add_edge(\"formulate_search_query\", \"search\")\n",
    "graph_builder.add_edge(\"search\", \"generate_answer\")\n",
    "graph_builder.add_edge(\"generate_answer\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the session state, we will use the [Bedrock Session API](https://docs.aws.amazon.com/bedrock/latest/userguide/sessions-opensource-library.html). The session API allows us to maintain a session with the LLM and keep track of the conversation state. This will help us maintain context across multiple turns of conversation.\n",
    "\n",
    "The open source [langgraph-checkpoint-aws](https://github.com/langchain-ai/langchain-aws/tree/main/libs/langgraph-checkpoint-aws) library provides a convenient abstraction for the Bedrock Session API that works directly with LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_checkpoint_aws.saver import BedrockSessionSaver\n",
    "\n",
    "memory = BedrockSessionSaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except Exception as e:\n",
    "    rprint(\"Issue with mermaid rendering API\")\n",
    "    mermaid_code = graph.get_graph().draw_mermaid()\n",
    "    rprint(\n",
    "        Markdown(\n",
    "            f\"To view the graph, copy the code below and paste it into a [mermaid live editor](https://mermaid.live/edit)\"\n",
    "        )\n",
    "    )\n",
    "    rprint(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new session id to store the state of the graph\n",
    "session_id = bedrock_agent_runtime.create_session()[\"sessionId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": session_id},}\n",
    "user_input = \"What are the best practices for model governance?\"\n",
    "res = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}], \"input_validated\": False}, config)\n",
    "rprint(res[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask a question that is not related to model risk\n",
    "user_input = \"How fast is a cheetah?\"\n",
    "res = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}], \"input_valid\": False}, config)\n",
    "rprint(res[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What are some example risk that I should document for a credit risk model?\"\n",
    "res = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}], \"input_valid\": False}, config)\n",
    "rprint(res[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow up on the previous question\n",
    "user_input = \"How about for a fraud detection model?\"\n",
    "res = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}], \"input_valid\": False}, config)\n",
    "rprint(res[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "You have reached the end of this workshop. Following cell will delete all created resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db.delete_index()\n",
    "rprint(\"Index deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In the above implementation of RAG based Question Answering and Conversational AI, we have explored the following concepts and how to implement them using the LangChain integrations for Amazon Bedrock and a local vector store:\n",
    "\n",
    "- Loading documents and processing them into smaller chunks\n",
    "- Creating a vector store using FAISS\n",
    "- Generating embeddings with an embeddings model\n",
    "- Searching the vector store to retrieve context relevant to the question\n",
    "- Performing Generative Question Answering using foundation models\n",
    "- Improving trust in our system by providing citations with every answer\n",
    "- Preparing prompt templates to use as input to the LLM\n",
    "- Storing conversation memory and providing the history as context to the LLM\n",
    "\n",
    "### Next steps\n",
    "- Experiment with different vector stores\n",
    "- Leverage various text and embedding models available through Amazon Bedrock to see alternate outputs\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Use Amazon Bedrock Knowledge Bases, a fully managed RAG capability with built-in session context management\n",
    "\n",
    "# Thank You"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
