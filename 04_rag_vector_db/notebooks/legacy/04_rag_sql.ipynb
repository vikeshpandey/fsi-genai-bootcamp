{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting insights from structured data with Text to SQL\n",
    "In this notebook we will explore another important modality of data, structured data, and how we can leverage it to extract insights from it. We will prompt the LLM to translate a user's question into a SQL query that runs against a database. Text to SQL is a task where the goal is to convert a natural language question into a SQL query that can be executed on a database to retrieve the answer. This task is particularly useful in the context of databases, where users can ask questions in natural language and get the answers in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment(\"sql_rag_requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete any prior artifacts\n",
    "!rm -f chroma.sqlite3 credit_scoring.duckdb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "The first step is to obtain the dataset and load it into a database for querying. We'll download an [OpenML credit loans](https://www.openml.org/search?type=data&status=active&id=45938&sort=runs) and then load it into a [DuckDB database](https://duckdb.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml(data_id=45938)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes as a single table which contains the customer, accounts, and monthly payment information. To make this more realistic, we will break the dataset into 3 tables one for each entity (customer, account, and monthly payment) and then create a foreign key relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_customers(df):\n",
    "    customers = (\n",
    "        df[[\"Customer_ID\", \"Name\", \"Age\", \"SSN\", \"Occupation\", \"Annual_Income\"]]\n",
    "        .drop_duplicates(\"Customer_ID\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return customers\n",
    "\n",
    "\n",
    "def extract_accounts(df):\n",
    "    accounts = (\n",
    "        df[\n",
    "            [\n",
    "                \"Customer_ID\",\n",
    "                \"Num_Bank_Accounts\",\n",
    "                \"Num_Credit_Card\",\n",
    "                \"Num_of_Loan\",\n",
    "                \"Type_of_Loan\",\n",
    "            ]\n",
    "        ]\n",
    "        .drop_duplicates(\"Customer_ID\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return accounts\n",
    "\n",
    "\n",
    "def extract_monthly_records(df):\n",
    "    monthly_records = df[\n",
    "        [\n",
    "            \"ID\",\n",
    "            \"Customer_ID\",\n",
    "            \"Month\",\n",
    "            \"Monthly_Inhand_Salary\",\n",
    "            \"Delay_from_due_date\",\n",
    "            \"Num_of_Delayed_Payment\",\n",
    "            \"Changed_Credit_Limit\",\n",
    "            \"Num_Credit_Inquiries\",\n",
    "            \"Credit_Mix\",\n",
    "            \"Outstanding_Debt\",\n",
    "            \"Credit_Utilization_Ratio\",\n",
    "            \"Credit_History_Age\",\n",
    "            \"Payment_of_Min_Amount\",\n",
    "            \"Total_EMI_per_month\",\n",
    "            \"Amount_invested_monthly\",\n",
    "            \"Payment_Behaviour\",\n",
    "            \"Monthly_Balance\",\n",
    "        ]\n",
    "    ]\n",
    "    return monthly_records\n",
    "\n",
    "\n",
    "customers_table = extract_customers(data)\n",
    "accounts_table = extract_accounts(data)\n",
    "monthly_records_table = extract_monthly_records(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset split into 3 tables, we can create a database and load the data into it. We will use DuckDB, a lightweight in-memory database that is easy to use and has good support for SQL queries.\n",
    "\n",
    "We do this by running DDL commands and ingesting the pandas dataframes into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "\n",
    "# Create or connect to a DuckDB database\n",
    "conn = duckdb.connect(\"credit_scoring.duckdb\")\n",
    "\n",
    "# Create tables and insert data into DuckDB\n",
    "\n",
    "# Customers Table\n",
    "customers_ddl = \"CREATE TABLE IF NOT EXISTS customers (Customer_ID VARCHAR, Name VARCHAR, Age INTEGER, SSN VARCHAR, Occupation VARCHAR, Annual_Income FLOAT)\"\n",
    "conn.execute(customers_ddl)\n",
    "conn.register(\"customers_df\", customers_table)\n",
    "conn.execute(\"INSERT INTO customers SELECT * FROM customers_df\")\n",
    "\n",
    "# Accounts Table\n",
    "accounts_ddl = \"CREATE TABLE IF NOT EXISTS accounts (Customer_ID VARCHAR, Num_Bank_Accounts INTEGER, Num_Credit_Card INTEGER, Num_of_Loan INTEGER, Type_of_Loan VARCHAR)\"\n",
    "conn.execute(accounts_ddl)\n",
    "conn.register(\"accounts_df\", accounts_table)\n",
    "conn.execute(\"INSERT INTO accounts SELECT * FROM accounts_df\")\n",
    "\n",
    "# Monthly Records Table\n",
    "records_ddl = \"CREATE TABLE IF NOT EXISTS monthly_records (ID VARCHAR, Customer_ID VARCHAR, Month VARCHAR, Monthly_Inhand_Salary FLOAT, Delay_from_due_date INTEGER, Num_of_Delayed_Payment INTEGER, Changed_Credit_Limit FLOAT, Num_Credit_Inquiries INTEGER, Credit_Mix VARCHAR, Outstanding_Debt FLOAT, Credit_Utilization_Ratio FLOAT, Credit_History_Age VARCHAR, Payment_of_Min_Amount VARCHAR, Total_EMI_per_month FLOAT, Amount_invested_monthly FLOAT, Payment_Behaviour VARCHAR, Monthly_Balance FLOAT)\"\n",
    "conn.execute(records_ddl)\n",
    "conn.register(\"monthly_records_df\", monthly_records_table)\n",
    "conn.execute(\"INSERT INTO monthly_records SELECT * FROM monthly_records_df\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the connection to the database and run a simple query to check if the data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect('credit_scoring.duckdb')\n",
    "query_result = conn.execute(\"SELECT * FROM customers WHERE Age > 30\").df()\n",
    "conn.close()\n",
    "\n",
    "query_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to SQL pipeline\n",
    "The next step is to build our Text to SQL pipeline. One of the easiest ways to do this is to leverage the [Vanna](https://github.com/vanna-ai/vanna) library. This library abstracts many of the patterns and best practices for building a Text to SQL pipeline and allows us to focus on the data and the model.\n",
    "\n",
    "Behind the scenes, Vanna uses Retrieval Augmented Generation (RAG) to generate the SQL query. It utilizes a vector database to store information about our data such as the schema, table names, and column names, example SQL statements, data documentation, and others. This information is then used to retrieve the most relevant information when generating the SQL query.\n",
    "\n",
    "Let's see how we can create a custom pipeline that leverages Bedrock and our DuckDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vanna.chromadb import ChromaDB_VectorStore\n",
    "from vanna.bedrock.bedrock_converse import Bedrock_Converse\n",
    "import boto3\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is where we configure our Vanna text to sql pipeline. We'll use Claude 3 models from Bedrock, a local [ChromaDB](https://www.trychroma.com/) vector database that is natively supported by Vanna, and our DuckDB database.\n",
    "\n",
    "There are a few modifications to get the best results with Claude3 models. First is the default `initial_prompt` is modified to instruct Claude to output the generated sql inot `<sql></sql>` xml tags. Secondly a custom `extract_sql` function is implemented to extract the generated sql from the xml tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "config = {\n",
    "    \"modelId\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"temperature\": 0.4,\n",
    "    \"max_tokens\": 2048,\n",
    "    \"initial_prompt\": f\"You are an ANSI SQL expert.\\n\"\n",
    "    + \"Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions.\\n\"\n",
    "    + \"Always place the generated SQL into <sql></sql> tags.\",\n",
    "}\n",
    "\n",
    "\n",
    "class MyVanna(ChromaDB_VectorStore, Bedrock_Converse):\n",
    "    def __init__(self, config=None):\n",
    "        ChromaDB_VectorStore.__init__(self, config=config)\n",
    "        Bedrock_Converse.__init__(self, client=bedrock_runtime, config=config)\n",
    "\n",
    "    def extract_sql(self, llm_response: str) -> str:\n",
    "        sql_text = re.search(r\"<sql>(.*?)</sql>\", llm_response, re.DOTALL)\n",
    "        if sql_text:\n",
    "            return sql_text.group(1)\n",
    "        else:\n",
    "            return llm_response\n",
    "\n",
    "\n",
    "vn = MyVanna(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the DuckDB database\n",
    "vn.connect_to_duckdb(url='duckdb:///credit_scoring.duckdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Training\" the RAG model\n",
    "We can now begin to \"train\" the model by ingesting information into the ChromaDB vector database. First, Vanna can generate some basic documentation for the tables in the database using a built-in template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_information_schema = vn.run_sql(\"SELECT * FROM INFORMATION_SCHEMA.COLUMNS\")\n",
    "\n",
    "# This will break up the information schema into bite-sized chunks that can be referenced by the LLM\n",
    "plan = vn.get_training_plan_generic(df_information_schema)\n",
    "plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the plan\n",
    "vn.train(plan=plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the generated documentation looks like\n",
    "training_data = vn.get_training_data()\n",
    "print(training_data.query(\"training_data_type == 'documentation'\").iloc[0][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a few more examples of training data using the DDL statements from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn.train(ddl=customers_ddl)\n",
    "vn.train(ddl=accounts_ddl)\n",
    "vn.train(ddl=records_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add additional documentation as well. For example, the only thing the LLM would know about the `Month` field is that it is a `VARCHAR`, but the month could be represented in any number of ways for example `January`, `Jan`, `01`, `01/24`, etc. We can add this information to the vector database to help the LLM generate the correct SQL query when it needs to filter by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn.train(documentation=\"In the records table, the column Month has the following values: January, February, March, April, May, June, July, August\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = vn.get_training_data()\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the database with natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a simple question\n",
    "sql, result, _ = vn.ask(question=\"What is the average age of the customers?\", visualize=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average age seems high, let's find out why\n",
    "sql, result, _ = vn.ask(question=\"Who are the top 5 oldest customers?\", visualize=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try another question\n",
    "sql, result, _ = vn.ask(question=\"Which customer makes the largest average monthly payment?\", visualize=False)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This question would have failed if we did not provide the additional documentation about the values in the Month column\n",
    "sql, result, _ = vn.ask(question=\"Which customer had the largest increase in monthly salary from 01 to 08? \", visualize=False, print_results=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a more complex query\n",
    "sql, result, _ = vn.ask(question=\"Create a binning of annual salary and analyze that against the average number of credit cards \", visualize=False, print_results=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have seen how to extract insights from structured data using Text to SQL. We have loaded a dataset into a database, created a Text to SQL pipeline using Vanna, and queried the database using natural language. This is a powerful tool that can be used to extract insights from structured data in a more intuitive way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
